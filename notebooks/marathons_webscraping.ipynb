{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping personal notebook. To scrape tables from website, and create DataFrames.\n",
    "- See notebook marathons_eda for exploration and nformation for final presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create for loop to get a single race   \n",
    "Write function to create for loop for multiple dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pages of results for each race. \n",
    "# From Readme file for class assignment (instructors got this info by using Postman app)\n",
    "# Need this info to build the loop and function later (in personal work) when build \n",
    "# function / for loop.\n",
    "\n",
    "pgs_full_2016 = 154   \n",
    "pgs_full_2017 = 147   \n",
    "pgs_full_2018 = 85   \n",
    "pgs_full_2019 = 113   \n",
    "pgs_half_2016 = 898   \n",
    "pgs_half_2017 = 892   \n",
    "pgs_half_2018 = 598   \n",
    "pgs_half_2019 = 690   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlbase_full_2019 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2019-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_full_2018 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2018-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_full_2017 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2017-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_full_2016 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2016-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_half_2019 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Half-Marathon/2019-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_half_2018 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Half-Marathon/2018-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_half_2017 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Half-Marathon/2017-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_half_2016 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Half-Marathon/2016-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The steps to get a DataFrame from one page of results look like this:\n",
    "STEP 1: Build a URL by combining the base url with a specific page number  \n",
    "STEP 2: Use requests.post() to get the results of the post  \n",
    "STEP 3: Make a soup from results.text  \n",
    "STEP 4: Look at the soup to identify the table you want based on one of its attributes (like class)   \n",
    "STEP 5: Pass the table as a string to pandas read_html()   \n",
    "STEP 6: What does that look like? What is the datatype?  \n",
    "STEP 7: Keep working with the data until you have it a DataFrame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2019-Results?gender=&agegroup=&bib=&firstname=&lastname=&page=99\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Build a URL by combining the base url with a specific page number\n",
    "\n",
    "base_f_19 = urlbase_full_2019\n",
    "page = 99             #To pull just one page to take a look at \n",
    "url_f_19 = base_f_19 + str(99)  #This appends page #99 to the end of the URL. \n",
    "print(url_f_19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a request using the `requests` [library](https://requests.readthedocs.io/en/master/user/quickstart/)\n",
    "- `request.get()` uses http GET to get a webpage\n",
    "- `request.post()` uses http POST when the webpage is submitting a form\n",
    "- checking the [`status_code`](https://www.restapitutorial.com/httpstatuscodes.html) on the result let's you know your request was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Use requests.post() to get the results of the post\n",
    "\n",
    "response_f_19 = requests.post(url_f_19)   #url defined above, it's page 99 (only) of 2019 full marathon.\n",
    "print(type(response_f_19))            #To see what type \"response\" is. It's requests.\n",
    "soup_f_19 = BS(response_f_19.text, 'lxml')   #parses response (page 99 of 2019 full marathon) into a soup\n",
    "print(type(soup_f_19))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Make a soup from results.text   This makes a new TABLE for full 2019 marathons\n",
    "\n",
    "tables_f_19 = soup_f_19.find_all('table',\n",
    "                  attrs = {'class': 'table table-responsive table-bordered'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 4: Look at the soup to identify the table you want based on one of its attributes (like class)\n",
    "\n",
    "len(tables_f_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Pass the table as a LIST to pandas read_html() \n",
    "\n",
    "# We can deduce that the first table is the one we want (position 0)\n",
    "#This code returns a LIST of the DATAFRAMES.. we're only asking for the one in position [0], \n",
    "#  so we'll just get one df.\n",
    "\n",
    "results_list_f_19 = pd.read_html(str(tables_f_19[0]))   #index position 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_list_f_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: What does that look like? What is the datatype?  It's a list\n",
    "\n",
    "print(type(results_list_f_19)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: This creates a df from the above list\n",
    "\n",
    "df_f_19 = results_list_f_19[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Bib</th>\n",
       "      <th>Name</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99999</td>\n",
       "      <td>32379</td>\n",
       "      <td>Raquel Flores</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99999</td>\n",
       "      <td>30292</td>\n",
       "      <td>Kyle Domingos</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99999</td>\n",
       "      <td>32850</td>\n",
       "      <td>Paul Dillard</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99999</td>\n",
       "      <td>31415</td>\n",
       "      <td>Nicole Bennett</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99999</td>\n",
       "      <td>32995</td>\n",
       "      <td>Rudy Novak</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Overall    Bib            Name      Time\n",
       "0    99999  32379   Raquel Flores  00:00:00\n",
       "1    99999  30292   Kyle Domingos  00:00:00\n",
       "2    99999  32850    Paul Dillard  00:00:00\n",
       "3    99999  31415  Nicole Bennett  00:00:00\n",
       "4    99999  32995      Rudy Novak  00:00:00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f_19.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT:  Creating for loop and function to scrape all tables for all races, then saving the resulting dfs as .csv files\n",
    "\n",
    "1. Create a for loop to pull all results for a single race\n",
    "    - build a complete url by combining a base url and page number (need to do once for each page, e.g. 113 times for 2019).\n",
    "    - TIP: As I'm building each line of the for loop, can print each one  to see what it does\n",
    "    - Use requests.post() to get the results of the post. This is necessary because the first table on the website is a form. The \"post\" tells it to run and give back page 1, then page 2, etc. \n",
    "        - If I wanted either of the otehr two tables of already-populated leaderboards, could use .get instead of .post\n",
    "    - Then make soup from the post\n",
    "    - Find the data we want from inside the soup. Pull in all things from the website named 'table' that have attributes 'class' : 'table table-responsive table-bordered'. That info. was gleaned from using Inspect on the website to look at the code.\n",
    "    - Pass the table as a string to pandas read_html()\n",
    "    \n",
    "Function notes from DataCamp:\n",
    "- When you DEFINE a function, write parameters in function header\n",
    "- When you CALL a function, you pass arguments into the function\n",
    "- Docstring: after writing def xxx(xxx), next line '''What the function does''' in plain language, for reference. Like '''Returns the square of a value'''\n",
    "- Top line, header, that specifies the function name and parameter(s) is the SIGNATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Bib</th>\n",
       "      <th>Name</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Scott Wietecha</td>\n",
       "      <td>02:25:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Brian Shelton</td>\n",
       "      <td>02:34:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1047</td>\n",
       "      <td>Christopher Capps</td>\n",
       "      <td>02:38:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1052</td>\n",
       "      <td>Jason Grimes</td>\n",
       "      <td>02:45:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1046</td>\n",
       "      <td>David Adams</td>\n",
       "      <td>02:47:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Overall   Bib               Name      Time\n",
       "0        1     1     Scott Wietecha  02:25:42\n",
       "1        2     3      Brian Shelton  02:34:43\n",
       "2        3  1047  Christopher Capps  02:38:43\n",
       "3        4  1052       Jason Grimes  02:45:06\n",
       "4        5  1046        David Adams  02:47:28"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a for loop to create a df for only 2016. \n",
    "# Preliminary step to creating generic for loop to use inside a function\n",
    "\n",
    "full_2016_df = pd.DataFrame()  \n",
    "for page in range(1, pgs_full_2016+1):     #getting pages 1 through end of pages from list of pgs at top\n",
    "    url = urlbase_full_2016 + str(page)    #page doest't have to be defined\n",
    "    response = requests.post(url)     #post info and take pg X of results\n",
    "    soup = BS(response.text, 'lxml')  #translates from http to soup, .text says only take the meat, not the packaging\n",
    "    tables = soup.find_all('table',\n",
    "                  attrs = {'class': 'table table-responsive table-bordered'})\n",
    "    results_list = pd.read_html(str(tables[0]))    #output of read_html is a list\n",
    "    df = results_list[0]\n",
    "    full_2016_df = full_2016_df.append(df)         #This takes the results thus far in df and appends next page.\n",
    "\n",
    "full_2016_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the function with generic terminology that gets defined in the SIGNATURE (two attributes in the first line)\n",
    "# Once this is created, it can be \"called\" in future cells, to run the for loop on a specific year.\n",
    "\n",
    "def scrape_marathons(page_count, urlbase):      #name is usually a verb\n",
    "    '''Returns a dataframe with all pages for a single race'''\n",
    "    \n",
    "    races_df = pd.DataFrame()  \n",
    "\n",
    "    for page in range(1, page_count+1):     #getting pages 1 through end of pages, as defined at top of notebook\n",
    "        url = urlbase + str(page)           #page doest't have to be defined\n",
    "        response = requests.post(url)       #post info and take pg X of results\n",
    "        soup = BS(response.text, 'lxml')    #translates from http to soup, .text says only take the meat, not the packaging\n",
    "        tables = soup.find_all('table',\n",
    "                      attrs = {'class': 'table table-responsive table-bordered'})\n",
    "        results_list = pd.read_html(str(tables[0]))    #output of read_html is a list\n",
    "        df = results_list[0]\n",
    "        races_df = races_df.append(df)             #every time thru appends 25 rows.\n",
    "    return races_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3842, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Already created 2016 df two cells above. No need to run the function on that.\n",
    "\n",
    "full_2016_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3651, 4)\n",
      "   Overall   Bib            Name      Time\n",
      "0        1     1  Scott Wietecha  02:40:25\n",
      "1        2  1145    Ryan Regnier  02:56:28\n",
      "2        3  1147  Daniel Everett  03:00:55\n",
      "3        4  1029   Khris Vickroy  03:04:47\n",
      "4        5  1119  Marcus Dilallo  03:04:53\n"
     ]
    }
   ],
   "source": [
    "# First use of \"scrape_marathons\" FUNCTION created above, \n",
    "# to pull all data for 8 DFs (4 half and 4 full marathons)\n",
    "\n",
    "full_2017_df = scrape_marathons(pgs_full_2017, urlbase_full_2017)\n",
    "print(full_2017_df.shape)\n",
    "print(full_2017_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2115, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_2018_df = scrape_marathons(pgs_full_2018, urlbase_full_2018)\n",
    "full_2018_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2811, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_2019_df = scrape_marathons(pgs_full_2019, urlbase_full_2019)\n",
    "full_2019_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22429, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_2016_df = scrape_marathons(pgs_half_2016, urlbase_half_2016)\n",
    "half_2016_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22292, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_2017_df = scrape_marathons(pgs_half_2017, urlbase_half_2017)\n",
    "half_2017_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14940, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_2018_df = scrape_marathons(pgs_half_2018, urlbase_half_2018)\n",
    "half_2018_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17236, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_2019_df = scrape_marathons(pgs_half_2019, urlbase_half_2019)\n",
    "half_2019_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next series of cells is to convert these dfs into .csv files\n",
    "These will be read into a new notebook for exploration, named marathons_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2016_df = full_2016_df.to_csv('full_2016.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2017_df = full_2017_df.to_csv('full_2017.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2018_df = full_2018_df.to_csv('full_2018.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2019_df = full_2019_df.to_csv('full_2019.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_2016_df = half_2016_df.to_csv('half_2016.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_2017_df = half_2017_df.to_csv('half_2017.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_2018_df = half_2018_df.to_csv('half_2018.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_2019_df = half_2019_df.to_csv('half_2019.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success! End of this notebook\n",
    "### EDA is being done in notebook named marathons_eda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
